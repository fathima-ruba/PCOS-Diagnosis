# -*- coding: utf-8 -*-
"""PCOS Diagnosis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F-2CGNFF65i8XpTXXH_LjBxrgINAsCT-
"""

import pandas as pd

# Load the Excel file into a DataFrame
pcos_df = pd.read_excel('PCOS_data_without_infertility.xlsx')

# Display some basic information and the first few rows of the DataFrame
pcos_df.info(), pcos_df.head()

# Load the second sheet from the Excel file into a DataFrame
pcos_df_sheet2 = pd.read_excel('PCOS_data_without_infertility.xlsx', sheet_name=1)

# Display some basic information and the first few rows of the DataFrame
pcos_df_sheet2.info(), pcos_df_sheet2.head()

"""It is a dataset with 541 entries and 45 columns. These columns include various features such as age, weight, height, BMI, blood group, pulse rate, and hormonal levels among others. The column PCOS (Y/N) presumably indicates whether the individual has PCOS or not, making it our target variable.

"""

# Check for missing values in the dataset
missing_values_count = pcos_df_sheet2.isnull().sum()
missing_values_count[missing_values_count > 0]

"""We have missing values in the following columns:

Marraige Status (Yrs): 1 missing value

Fast food (Y/N): 1 missing value

Unnamed: 44: 539 missing values (almost all entries are missing)

For the first two columns with only 1 missing value, we could either remove those rows or fill them with an appropriate statistic like the mean or median.

The column Unnamed: 44 has almost all values missing, so it might be best to drop it.
"""

# Drop the column with almost all missing values ('Unnamed: 44')
pcos_df_cleaned = pcos_df_sheet2.drop(columns=['Unnamed: 44'])

# Fill the missing values in the remaining columns with their median values
pcos_df_cleaned['Marraige Status (Yrs)'].fillna(pcos_df_cleaned['Marraige Status (Yrs)'].median(), inplace=True)
pcos_df_cleaned['Fast food (Y/N)'].fillna(pcos_df_cleaned['Fast food (Y/N)'].median(), inplace=True)

# Verify if all missing values have been handled
pcos_df_cleaned.isnull().sum().sum()

# Identify categorical variables in the dataset
categorical_columns = pcos_df_cleaned.select_dtypes(include=['object']).columns
categorical_columns

# Inspect unique values in the categorical columns
unique_values_categorical = {col: pcos_df_cleaned[col].unique() for col in categorical_columns}
unique_values_categorical

# Convert columns to numeric, coercing non-numeric values to NaN
pcos_df_cleaned['II    beta-HCG(mIU/mL)'] = pd.to_numeric(pcos_df_cleaned['II    beta-HCG(mIU/mL)'], errors='coerce')
pcos_df_cleaned['AMH(ng/mL)'] = pd.to_numeric(pcos_df_cleaned['AMH(ng/mL)'], errors='coerce')

# Replace NaN values with median of the column
pcos_df_cleaned['II    beta-HCG(mIU/mL)'].fillna(pcos_df_cleaned['II    beta-HCG(mIU/mL)'].median(), inplace=True)
pcos_df_cleaned['AMH(ng/mL)'].fillna(pcos_df_cleaned['AMH(ng/mL)'].median(), inplace=True)

# Verify if all non-numeric values have been replaced
pcos_df_cleaned[['II    beta-HCG(mIU/mL)', 'AMH(ng/mL)']].isnull().sum()

"""# Feature Engineering"""

# 1. BMI Categories: Convert BMI into categories like underweight, normal, overweight, and obese.
def categorize_bmi(bmi):
    if bmi < 18.5:
        return 'Underweight'
    elif 18.5 <= bmi < 24.9:
        return 'Normal'
    elif 25 <= bmi < 29.9:
        return 'Overweight'
    else:
        return 'Obese'

pcos_df_cleaned['BMI Category'] = pcos_df_cleaned['BMI'].apply(categorize_bmi)

# 2. Age Groups: Create age categories
def categorize_age(age):
    if age < 18:
        return 'Teenager'
    elif 18 <= age < 30:
        return 'Young Adult'
    elif 30 <= age < 50:
        return 'Adult'
    else:
        return 'Senior'

pcos_df_cleaned['Age Group'] = pcos_df_cleaned[' Age (yrs)'].apply(categorize_age)

# 3. Hormonal Ratios: Create new features representing ratios between different hormone levels
pcos_df_cleaned['FSH/LH Ratio'] = pcos_df_cleaned['FSH(mIU/mL)'] / pcos_df_cleaned['LH(mIU/mL)']

# 4. Blood Pressure Categories: Convert blood pressure readings into categories
def categorize_bp(systolic, diastolic):
    if systolic < 120 and diastolic < 80:
        return 'Normal'
    elif 120 <= systolic < 130 and diastolic < 80:
        return 'Elevated'
    elif 130 <= systolic < 140 or 80 <= diastolic < 90:
        return 'Hypertension Stage 1'
    elif 140 <= systolic or 90 <= diastolic:
        return 'Hypertension Stage 2'
    else:
        return 'Hypertensive Crisis'

pcos_df_cleaned['Blood Pressure Category'] = pcos_df_cleaned.apply(lambda row: categorize_bp(row['BP _Systolic (mmHg)'], row['BP _Diastolic (mmHg)']), axis=1)

# Show first few rows to verify changes
pcos_df_cleaned.head()

"""We've added four new columns:

BMI Category: Categorizes BMI into underweight, normal, overweight, and obese.

Age Group: Categorizes age into teenager, young adult, adult, and senior.

FSH/LH Ratio: The ratio between FSH and LH hormone levels.

Blood Pressure Category: Categorizes blood pressure into normal, elevated, hypertension stage 1, and hypertension stage 2.
"""

from sklearn.model_selection import train_test_split

# Drop the columns that are not features (e.g., IDs, target variable)
X = pcos_df_cleaned.drop(columns=['Sl. No', 'Patient File No.', 'PCOS (Y/N)'])

# One-hot encode categorical features
X = pd.get_dummies(X, columns=['BMI Category', 'Age Group', 'Blood Pressure Category'], drop_first=True)

# Target variable
y = pcos_df_cleaned['PCOS (Y/N)']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Show the shape of the resulting sets
X_train.shape, X_test.shape, y_train.shape, y_test.shape

"""The data has been successfully split into training and testing sets.

432 samples in the training set with 49 features.

109 samples in the testing set with 49 features.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score
import numpy as np

# Initialize models
models = [
    ('Logistic Regression', LogisticRegression(max_iter=1000, random_state=42)),
    ('Decision Tree', DecisionTreeClassifier(random_state=42)),
    ('Random Forest', RandomForestClassifier(random_state=42)),
    ('SVM', SVC(random_state=42)),
    ('k-NN', KNeighborsClassifier()),
    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))
]

# Evaluate each model using cross-validation
cv_results = {}
for name, model in models:
    cv_score = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')
    cv_results[name] = np.mean(cv_score)

cv_results

"""After evaluating the models using cross-validation, we find the following average accuracy scores:

Logistic Regression: 87.03

Decision Tree: 80.55

Random Forest: 90.04

SVM (Support Vector Machines): 66.43

k-NN (k-Nearest Neighbors): 66.43

Gradient Boosting: 89.81

The Random Forest model has the highest average accuracy score of 90.04, closely followed by Gradient Boosting at 89.81.
"""

from sklearn.metrics import accuracy_score, classification_report

# Initialize and train the Random Forest model with default parameters
rf_default = RandomForestClassifier(random_state=42)
rf_default.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_default.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

accuracy, classification_rep

"""The Random Forest model with default settings achieved an accuracy of approximately 86.24%.

# Testing
"""

import random

# Generate synthetic sample data for demonstration
sample_data = {
    'Age (yrs)': [random.randint(20, 40)],
    'Weight (Kg)': [random.uniform(50.0, 80.0)],
    'Height(Cm) ': [random.uniform(150.0, 180.0)],
    # ... (populate all necessary features)
}

# Convert to DataFrame
sample_df = pd.DataFrame(sample_data)

# Perform the same feature engineering as done on the original dataset
# For demonstration, we'll assume the same transformations are applied
# sample_df_transformed = perform_feature_engineering(sample_df)

# One-hot encode categorical features
# For demonstration, we'll assume the same transformations are applied
# sample_df_encoded = pd.get_dummies(sample_df_transformed)

# NOTE: In a real-world scenario, the sample data should undergo the exact same preprocessing and feature engineering steps
# as the original dataset. For demonstration purposes, we'll directly use the first row from the original dataset.

sample_for_prediction = X.iloc[0].values.reshape(1, -1)

# Make a prediction
predicted_class = rf_default.predict(sample_for_prediction)
predicted_class

"""The Random Forest model predicts that the sample data belongs to class
0, which, according to the original dataset, indicates that the individual does not have Polycystic Ovary Syndrome (PCOS).
"""

sample = X[y == 1].iloc[0].values.reshape(1, -1)

# Make a prediction using the trained Random Forest model
predicted_class_pcos = rf_default.predict(sample)
predicted_class_pcos

"""The Random Forest model predicts that the sample data belongs to class
1, which, according to the original dataset, indicates that the individual has Polycystic Ovary Syndrome (PCOS).

This shows that the model is capable of identifying cases of PCOS based on the features provided.
"""